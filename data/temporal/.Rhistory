##### FUNCTION TO DOWNLOAD MENE DATA AND CREATE A TIBBLE OF UPPER TIER LA's WITH AVERAGE ANNUAL VISIT COUNTS TO THEM #####
read.menedata <- function(){
require(tidyverse)
require(haven)
# read in mene - visit file - where each row is a visit rather than a respondent
mene <- read_sav("http://publications.naturalengland.org.uk/file/6746346730291200")
# create tibble of annual average unweighted and weighted counts of visits
mene %>%
mutate(new_weight=if_else(year<=7, ConvertedWeekVweight, ConvertedMonthVweight)) %>%
# have to do some weights workarounds for weighted data because of changes to question frequency from year 8 onwards
# these are the correct ones for county (upper tier LA) level according to NE's guidance
group_by(DESTINATION_UPPERTIER_LOCALAUTHORITY) %>%
summarise(average_annual_unweighted_count=n()/10,
average_annual_weighted_count=sum(new_weight)*1000/10) %>% # because all weights should be read as thousands and /10 as 10 years of data
mutate(DESTINATION_UPPERTIER_LOCALAUTHORITY=as_factor(DESTINATION_UPPERTIER_LOCALAUTHORITY)) %>% # to remove labelled class
ungroup()
}
##### END #####
read.menedata()
##### FUNCTION TO DOWNLOAD MENE DATA AND CREATE A TIBBLE OF UPPER TIER LA's WITH AVERAGE ANNUAL VISIT COUNTS TO THEM #####
read.naturalenglandmene <- function(){
require(tidyverse)
require(haven)
# read in mene - visit file - where each row is a visit rather than a respondent
mene <- read_sav("http://publications.naturalengland.org.uk/file/6746346730291200")
# create tibble of annual average unweighted and weighted counts of visits
mene %>%
mutate(new_weight=if_else(year<=7, ConvertedWeekVweight, ConvertedMonthVweight)) %>%
# have to do some weights workarounds for weighted data because of changes to question frequency from year 8 onwards
# these are the correct ones for county (upper tier LA) level according to NE's guidance
group_by(DESTINATION_UPPERTIER_LOCALAUTHORITY) %>%
summarise(average_annual_unweighted_count=n()/10,
average_annual_weighted_count=sum(new_weight)*1000/10) %>% # because all weights should be read as thousands and /10 as 10 years of data
mutate(DESTINATION_UPPERTIER_LOCALAUTHORITY=as_factor(DESTINATION_UPPERTIER_LOCALAUTHORITY)) %>% # to remove labelled class
ungroup()
}
##### END #####
x<-read.naturalenglandmene
View(x)
View(x)
x
x<-read.naturalenglandmen()
x<-read.naturalenglandmene()
View(x)
relative2baseline.met<-function(){
# Load packages ----------------------------------------------------------
#install.packages('plotrix')
library(plotrix)
#install.packages('tibble')
library(tibble)
#install.packages('ggplot2')
library(ggplot2)
#install.packages('gridExtra')
library(gridExtra)
#install.packages('grid)
library(grid)
#install.packages('ggplot2)
library(ggplot2)
#install.packages('lattice')
library(lattice)
#install.packages("tidyr")
library(tidyr)
#install.packages('reshape2')
library(reshape2)
#install.packages('dplyr')
library(dplyr)
#install.packages('XML')
library(XML) # HTML processing
#install.packages('RCurl')
library(RCurl)
#install.packages('rvest')
library(rvest)
#install.packages('stringr')
library(stringr)
#install.packages('plotrix)
library(plotrix)
source('match.metoffice2google.R')
# read in met office england dataset and add column for weekdays -------------------------------------------------------------------
metoffice_england<-match.metoffice2google()
#create vector of days of the week and add to dataframe
metoffice_england<-metoffice_england %>% tibble::add_column(weekdays = weekdays(as.Date(metoffice_england$date)),
.before = 1)
# make relative to baseline -----------------------------------------------
#make a vector of dates within Google's non-baseline/time series period
nonbaselinerange<-seq(as.Date('2020-02-15'),as.Date('2020-06-27'),1)
#get met office england data for the nonbaseline period
metoffice_england_nonbaselineperiod<-metoffice_england[as.Date(metoffice_england$date)%in%nonbaselinerange,]
#make a vector of dates within Google's Baseline period
baselinerange<-seq(as.Date('2020-01-03'),as.Date('2020-02-06'),1)
#get met office england data for the baseline period
metoffice_england_baselineperiod<-metoffice_england[as.Date(metoffice_england$date)%in%baselinerange,]
unique(as.Date(metoffice_england$date))
unique(as.Date(baselinerange))
#create a dataframe of the median values of each meteorological measurement for each district, from the baseline period (as google did for mobility)
baselineweather<-aggregate(metoffice_england_baselineperiod,
list(metoffice_england_baselineperiod$weekdays,
metoffice_england_baselineperiod$sub_region_1),
median)[,-c(1:2)]
#make vector of weekdays
wdays<-c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
#make vector of districts
districts<-levels(as.factor(metoffice_england$sub_region_1))
#copy df to new df which will be relative to baseline df
metoffice_england_rel2baseline<-metoffice_england_nonbaselineperiod
#This loop calculates the 'baseline change in weather' using Google's method applied to the Met Office data. Process:
#1. Go through each weekday (first would be Monday)
#2. Go through each Google district (first would be Bath and Northeast Somerset)
#3. Go through each of the meteorological variables in columns 4:53 in the Met Office dataframe (first would be air temp mean max K)
#4. Nonbaseline - A temporary object - in first instance, would be all the Met Office air temp mean max K measurements from all the Mondays from Bath and Northeast Somerset in the non-baseline time series
#5. Baseline - A temporary object - in first instance, would be all the Met Office air temp mean max K measurements from all the Mondays from Bath and Northeast Somerset in the baseline time series
#6. Diff_from_baseline - A temporary obect - Subtract baseline from nonbaseline to get absolute change in that variable per weekday per district (as in Google methods)
#7. Write to the relative change dataframe in the appropriate columns - Make the difference relative by dividing it by the baseline and multiplying by 100 to get in percent as for Google data
#loop through weekdays
for (w in wdays){
print(w)
#loop through districts
for(d in districts){
print(d)
#loop through columns
for (c in 4:ncol(metoffice_england_rel2baseline)){
print(c)
#for weekday w and district w, get non-baseline median values of meteorological measurement c for
nonbaseline<-metoffice_england_nonbaselineperiod[metoffice_england_nonbaselineperiod$weekdays==w & metoffice_england_nonbaselineperiod$sub_region_1==d,c]
#for weekday w and district w, get baseline median values of meteorological measurement c
baseline<-baselineweather[baselineweather$weekdays==w & baselineweather$sub_region_1==d,c]
#subtract the baseline values of meteorological measurement c for weekday w and district w
diff_from_baseline<-nonbaseline-baseline
#object is the 'nonbaseline rows in the new df (rel2baseline)
#function is to divide the difference from baseline by the baseline to get relative difference the times by 100 to get in percent
metoffice_england_rel2baseline[metoffice_england_nonbaselineperiod$weekdays==w & metoffice_england_nonbaselineperiod$sub_region_1==d,c]<-diff_from_baseline/baseline*100
}
}
}
metoffice_england_rel2baseline
}
#DEFAULT SETTINGs TO PARKS and BEDFORD (Data stil needs to be unputted)
plot.googlemobilitydistricts<-function(Data,location="parks",district="Bedford"){
# Load packages ----------------------------------------------------------
#install.packages('plotrix')
library(plotrix)
#install.packages('tibble')
library(tibble)
#install.packages('ggplot2')
library(ggplot2)
#install.packages('gridExtra')
library(gridExtra)
#install.packages('grid)
library(grid)
#install.packages('ggplot2)
library(ggplot2)
#install.packages('lattice')
library(lattice)
#install.packages("tidyr")
library(tidyr)
#install.packages('reshape2')
library(reshape2)
#install.packages('dplyr')
library(dplyr)
#install.packages('plyr')
library(plyr)
#install.packages('XML')
library(XML) # HTML processing
#install.packages('RCurl')
library(RCurl)
#install.packages('rvest')
library(rvest)
#install.packages('stringr')
library(stringr)
#install.packages('plotrix)
library(plotrix)
#Creates a character vector that resembles the column name within the google mobility data.
Loc<-paste(location,"_percent_change_from_baseline",sep = "")
#Creates a smaller data set that only corresponds to the values needed.
district_data<-subset(Data,select = c("date","sub_region_1",Loc))
#Makes many data frames for each district.
district_all<-split(district_data,district_data$sub_region_1)
#Makes a data frame of the single district required.
district_df<-as.data.frame(district_all[district])
#Changes the column names for making it easier to put into a plotting function.
colnames(district_df)<-c("Date","District","Mobility")
#Cleans the location title to ensure that _ do not exist in the y axis and recreates the y-axis.
country_ylab<-paste("Visit changes for",gsub("_"," ",location),"(%) relative to per-weekday winter baselines (Google Community Mobility data)")
#START OF THE PLOTTING FUNCTION
District_graph<-ggplot(data=district_df,aes(x=as.Date(Date),y=Mobility)) +
#Plots the bar graphs, with a black outing and dark orange fill.
geom_col(position = position_dodge(width=0.2), size=0.25,colour = 'black', fill ='#D55E00') +
#Limits the size of the graph.
coord_cartesian(ylim=c(-100,160)) +
#plots a horizontal line where no percentage change occurs.
geom_hline(yintercept=0) +
#Ensure the background is white, the border is black and removes grid lines.
theme(panel.background = element_rect(fill = "white", colour = "black", size = 1, linetype = "solid"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
strip.text = element_blank())+
#x-label
xlab("Date") +
#y-label using the previous clean code done outside the plot.
ylab(country_ylab)+
#Add a title for the district data this graph represents.
ggtitle(district)
District_graph
}
setwd("C:/Users/mlj206/Github/parksinthepandemic/data/temporal")
plot.googlemobilitydistricts('google_and_metoffice.csv')
plot.googlemobilitydistricts(read.csv('google_and_metoffice.csv'))
