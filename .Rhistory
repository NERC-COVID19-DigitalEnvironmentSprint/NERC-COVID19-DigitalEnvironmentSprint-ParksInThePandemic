View(plot.googlemobilitydistricts)
runApp()
install.packages(c("conflicted", "osfr"))
shiny::runApp()
runApp()
rlang::last_error()
runApp()
setwd("C:/Users/anna-/OneDrive - University of Exeter/parksinthepandemic")
runApp()
runApp()
runApp()
rlang::last_error()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
setwd("C:/Users/anna-/OneDrive - University of Exeter/parksinthepandemic")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp('C:/Users/anna-/OneDrive - University of Exeter/Diag_App/Simuland')
runApp('C:/Users/anna-/OneDrive - University of Exeter/parksinthepandemic')
runApp()
setwd("C:/Users/mlj206/Github/parksinthepandemic")
library(osfr)
# Authenticate with full access token
osf_auth(readLines("./../osfauth.txt", n=1))
# READ IN GOOGLE DATA & WRITE TO OSF --------------------------------------------------------
#make temporary directory
dir.create("./temporary")
source('code/read.googlemobility.R')
#google community mobility reports
google<-read.googlemobility()
google_england<-subset(google,sub_country=='England')
write.csv(google_england, 'temporary/google_england.csv', row.names = F)
# Get tibble of data on the OSF store
pp_project <- osf_ls_files(osf_retrieve_node("c7kg4"))
# Get latest temporal directory
temporal<-osf_ls_files(pp_project[which(pp_project$name=='temporal'),])
temporal
#remove existing google_england file (workaround as overwrite doesn't work right now)
osf_rm(temporal[which(temporal$name=='google_england.csv'),], check=FALSE)
#write new version in it's place
osf_upload(pp_project[which(pp_project$name=='temporal'),],
'temporary/google_england.csv')
#read
source('code/read.metofficecovid.R')
#met office covid data
metoffice_england<-read.metofficecovid()
#match
source('code/match.metoffice.R')
#match met office with google data and rename columns to fit OpenWeather forecast format
metoffice_england<-match.metoffice(metoffice_england,google_england)
write.csv(metoffice_england, 'temporary/metoffice_england.csv', row.names=F)
#write
# Get tibble of data on the OSF store
pp_project <- osf_ls_files(osf_retrieve_node("c7kg4"))
# Get latest temporal directory
temporal<-osf_ls_files(pp_project[which(pp_project$name=='temporal'),])
temporal
#remove existing google_england file (workaround as overwrite doesn't work right now)
osf_rm(temporal[which(temporal$name=='metoffice_england.csv'),], check=FALSE)
#write new version in it's place
osf_upload(pp_project[which(pp_project$name=='temporal'),],
'temporary/metoffice_england.csv')
# MAKE WEATHER DATA RELATIVE TO BASELINE AND MERGE WITH GOOGLE ----------------------------------
source('code/relative2baseline.R')
metoffice_england_r2b<-relative2baseline(metoffice_england)
googleandmetoffice_england<-merge(google_england,metoffice_england_r2b, all=T)
write.csv(googleandmetoffice_england, 'temporary/googleandmetoffice_england.csv')
#write
# Get tibble of data on the OSF store
pp_project <- osf_ls_files(osf_retrieve_node("c7kg4"))
# Get latest temporal directory
temporal<-osf_ls_files(pp_project[which(pp_project$name=='temporal'),])
temporal
#remove existing google_england file (workaround as overwrite doesn't work right now)
osf_rm(temporal[which(temporal$name=='googleandmetoffice_england.csv'),], check=FALSE)
#write new version in it's place
osf_upload(pp_project[which(pp_project$name=='temporal'),],
'temporary/googleandmetoffice_england.csv')
#delete temporary directory
unlink("./temporary", recursive=T)
#make data directory
dir.create("./data")
#download latest data just uploaded to OSF
pp_project <- osf_ls_files(osf_retrieve_node("c7kg4"))
osf_download(pp_project[which(pp_project$name%in%c('temporal','singlemeasure', 'model')),], path="./data", verbose=TRUE, progress=TRUE, recurse=TRUE, conflicts="overwrite")
source('code/create.model.R')
#read in freshly downloaded model inputs
googleandmetoffice_england<-read.csv('data/temporal/googleandmetoffice_england.csv')
mene_england<-read.csv('data/singlemeasure/mene_england.csv')
gardenaccess_england<-read.csv('data/singlemeasure/gardenaccess_england.csv')
#rerun model
RF_model<-create.model(googleandmetoffice_england, mene_england, gardenaccess_england)
#save model locally
saveRDS(RF_model,'data/model/RF_model.RDS')
# Get latest model directory
model<-osf_ls_files(pp_project[which(pp_project$name=='model'),])
model
#remove existing google_england file (workaround as overwrite doesn't work right now)
osf_rm(model[which(model$name=='RF_model.RDS'),], check=FALSE)
#write new version in it's place
osf_upload(pp_project[which(pp_project$name=='model'),],
'data/model/RF_model.RDS')
source('code/getandmatch.forecast.R')
districts<-readRDS('code/input_data/google_englanddistricts.RDS')
forecasts_england<-c()
#loops through each district to get forecasts, pausing for 1 second in between each one so doesn't overload OpenWeather API
for (d in districts){
currentforecast<-cbind(sub_region_1 = d, getandmatch.forecast(d))
forecasts_england<-rbind(currentforecast,forecasts_england)
Sys.sleep(1)
}
#Make forecasts_england relative to baseline.
forecasts_england<-relative2baseline(forecasts_england)
#write forecasts to local directory
write.csv(forecasts_england,'data/model/forecasts_england.csv')
# Get latest model directory
model<-osf_ls_files(pp_project[which(pp_project$name=='model'),])
model
#remove existing google_england file (workaround as overwrite doesn't work right now)
osf_rm(model[which(model$name=='forecasts_england.csv'),], check=FALSE)
#write new version in it's place
osf_upload(pp_project[which(pp_project$name=='model'),],
'data/model/forecasts_england.csv')
#delete data directory
unlink("./data", recursive=T)
# Get tibble of data on the OSF store
data <- osfr::osf_ls_files(osf_retrieve_node("c7kg4"))
# Download to ./data
dir.create("./data")
osf_download(data, path="./data", verbose=TRUE, progress=TRUE, recurse=TRUE, conflicts="overwrite")
240/5
View(forecasts_england)
districts
source('code/getandmatch.forecast.R')
districts<-readRDS('code/input_data/google_englanddistricts.RDS')
forecasts_england<-c()
#loops through each district to get forecasts, pausing for 1 second in between each one so doesn't overload OpenWeather API
for (d in districts){
currentforecast<-cbind(sub_region_1 = d, getandmatch.forecast(d))
forecasts_england<-rbind(currentforecast,forecasts_england)
Sys.sleep(2)
}
warnings()
districts
240/6
d
85/2
d
currentforecast<-cbind(sub_region_1 = d, getandmatch.forecast(d))
getandmatch.forecast(d)
getandmatch.forecast(d)
getandmatch.forecast(d)
d
getandmatch.forecast(d)
# Inserting in the open weather data --------------------------------------
#Saving location for later
end_location<-'Merseyside'
#Retrieves the APIkey from the offline Github folder containing your cloned repos
APIkey<-apikey
apikey=readRDS("./../APIkey.RDS")
location='Merseyside'
# Inserting in the open weather data --------------------------------------
#Saving location for later
end_location<-location
#Retrieves the APIkey from the offline Github folder containing your cloned repos
APIkey<-apikey
#Set's the system to my own API key.
Sys.setenv(OWM_API_KEY = APIkey)
#Convert location, if necessary
location_1<-ifelse(location == "Greater Manchester", location <- c("Bolton","Bury","Manchester","Oldham","Rochdale","Salford","Tameside","Trafford","Wigan"),
ifelse(location == "Tyne and Wear", location <- c("Gateshead","Newcastle upon Tyne","South Tyneside","Sunderland","Holystone"),
ifelse(location == "Merseyside", location <- c("Knowsley","Liverpool","Sefton","St. Helens","Stockport","Wirral"),
ifelse(location == "West Yorkshire", location <-c("Bradford","Calderdale","Kirklees","Leeds","Wakefield"),
ifelse(location == "South Yorkshire", location <- c("Barnsley","Doncaster","Rotherham","Sheffield"),
ifelse(location == "West Midlands", location <- c("Birmingham","Coventry","Dudley","Sandwell","Solihull","Telford and Wrekin","Walsall","Wolverhampton"),
ifelse(location == "Windsor and Maidenhead", location <- c("Windsor","Maidenhead"),
ifelse(location == 'Bristol City', location <- c('Bristol'),
location))))))))
#Get's the forecast and creates a tibble for the first location in the location vector.
forecast <-owmr::get_forecast(location[1], units = "metric")%>%
owmr_as_tibble()%>%
#Then adds a column for it's designated sub_region_1.
tibble::add_column(sub_region_1 = location[1],.after = "dt_txt")
#If the location vector is above 1, it adds the additional location data into the forecasting data frame.
if(length(location) > 1){
forecast<-forecast
for (i in 2:length(location)) {
forecast_1<-owmr::get_forecast(location[i], units = "metric")%>%
owmr_as_tibble()%>%
tibble::add_column(sub_region_1= location[i], .after = "dt_txt")
forecast<-rbind(forecast,forecast_1)
}
}
forecast_1
View(forecast_1)
forecast
View(forecast_1)
View(forecast)
View(forecast_1)
match(colnames(forecast,forecast_1))
match(colnames(forecast),colnames(forecast_1))
#If the location vector is above 1, it adds the additional location data into the forecasting data frame.
if(length(location) > 1){
forecast<-forecast
for (i in 2:length(location)) {
forecast_1<-owmr::get_forecast(location[i], units = "metric")%>%
owmr_as_tibble()%>%
tibble::add_column(sub_region_1= location[i], .after = "dt_txt")
forecast<-merge(forecast,forecast_1)
}
}
View(forecast)
?merge
#If the location vector is above 1, it adds the additional location data into the forecasting data frame.
if(length(location) > 1){
forecast<-forecast
for (i in 2:length(location)) {
forecast_1<-owmr::get_forecast(location[i], units = "metric")%>%
owmr_as_tibble()%>%
tibble::add_column(sub_region_1= location[i], .after = "dt_txt")
forecast<-merge(forecast,forecast_1, all=T)
}
}
View(forecast)
source('code/getandmatch.forecast.R')
districts<-readRDS('code/input_data/google_englanddistricts.RDS')
forecasts_england<-c()
#loops through each district to get forecasts, pausing for 1 second in between each one so doesn't overload OpenWeather API
for (d in districts){
currentforecast<-cbind(sub_region_1 = d, getandmatch.forecast(d))
forecasts_england<-rbind(currentforecast,forecasts_england)
Sys.sleep(1)
}
516/6
#Make forecasts_england relative to baseline.
forecasts_england<-relative2baseline(forecasts_england)
#write forecasts to local directory
write.csv(forecasts_england,'data/model/forecasts_england.csv')
# Get latest model directory
model<-osf_ls_files(pp_project[which(pp_project$name=='model'),])
model
#remove existing google_england file (workaround as overwrite doesn't work right now)
osf_rm(model[which(model$name=='forecasts_england.csv'),], check=FALSE)
#write new version in it's place
osf_upload(pp_project[which(pp_project$name=='model'),],
'data/model/forecasts_england.csv')
#delete data directory
unlink("./data", recursive=T)
# Get tibble of data on the OSF store
data <- osfr::osf_ls_files(osf_retrieve_node("c7kg4"))
# Download to ./data
dir.create("./data")
osf_download(data, path="./data", verbose=TRUE, progress=TRUE, recurse=TRUE, conflicts="overwrite")
exits(forecast_1$rain_3h)
exists(forecast_1$rain_3h)
forecast_1
forecast_1$rain_3h
exists(forecast_1$rain_3h)
rain_3h %in% colnames(forecast_1)
colnames(forecast_1)
'rain_3h' %in% colnames(forecast_1)
is.na(forecast_1$rain_3h)
if('rain_3h' %in% colnames(forecast_1),is.na(forecast_1$rain_3h)<-0)
ifelse('rain_3h' %in% colnames(forecast_1), is.na(forecast_1$rain_3h)<-0,
forecast<-merge(forecast,forecast_1, all=T)
}
}
#This extracts the date and time column into two separate strings.
date_time<-t(as.data.frame(strsplit(forecast$dt_txt," ")))
#Then combines them to the main data frame.
forecast<-as.data.frame(cbind("date" = (as.Date(date_time[,1])), "time" = date_time[,2],forecast))[,-3]
#Selects a subset of the forecast dataframe for easing transformations of the table.
forecast<-subset(forecast, select = c("date","sub_region_1","temp","temp_min","temp_max","rain_3h"))
#Converts the rain per 3 hours into rain per hour to be compatible with the metoffice.
forecast<-forecast%>%
dplyr::mutate(
rain_3h = rain_3h/3
)
#If location is not equal to the end location (i.e. it's a vector) then it will aggregate all the values
ifelse (location != end_location,
forecast_2<-aggregate(forecast[,3:length(forecast)], list(forecast$date,forecast$sub_region_1), FUN = mean, na.rm = T ),
forecast_2<-forecast
)
#This renames the column names as the above code will change the column names if an aggregation has taken place.
colnames(forecast_2)<-c("date","sub_region_1","temp","temp_min","temp_max","rain_3h")
#This calculates the aggregation value for mean temperature by taking the mean.
forecast_temp_mean<-aggregate(forecast_2$temp, list(forecast_2$date), FUN = mean, na.rm = T )
#This calculates the max temperature by taking the max.
forecast_temp_max<-aggregate(forecast_2$temp_max, list (forecast_2$date), FUN = max, na.rm = T)
#This calculates the min temperature by taking the min.
forecast_temp_min<-aggregate(forecast_2$temp_min, list (forecast_2$date), FUN = min, na.rm = T)
#This calculates the aggregation value for the mean rainfall per hour by taking the mean.
forecast_rain_mean<-aggregate(forecast_2$rain_3h, list (forecast_2$date), FUN = mean, na.rm = T)
#This combines the above vectors created and adds weekdays for each value, date, and creates a column for sub_region_1 based from the first definition.
forecast_2<-cbind.data.frame(
"date" = forecast_temp_mean$Group.1,
'weekday' = weekdays(forecast_temp_mean$Group.1),
"temp_mean" = forecast_temp_mean$x,
"temp_max" = forecast_temp_max$x,
"temp_min" =  forecast_temp_min$x,
"rain_mean" = forecast_rain_mean$x)
#change NaNs to NAs in dataframe
forecast_2[do.call(cbind, lapply(forecast_2,is.nan))]<-NA
#Reads in menedata
mene_england = read.csv("code/input_data/testdata/mene_england.csv")
#Reads in garden_access data
garden_access = read.csv("code/input_data/testdata/garden_access.csv")
#Removes one column that is not required for modelling from garden_access.
garden_access<-garden_access[-3]
#Combines the appropriate location information from both datasets to the forecast datatset and removes another column from garden_access.
forecast_2<-cbind.data.frame(forecast_2,subset(garden_access[-4],google_district == end_location)[,-1],"annual_visits_per_capita_per_km2_greenspace_1km_radius" = subset(mene_england,sub_region_1 == end_location)[,4])
}
ifelse('rain_3h' %in% colnames(forecast_1), is.na(forecast_1$rain_3h)<-0,)
ifelse('rain_3h' %in% colnames(forecast_1), is.na(forecast_1$rain_3h)<-0,forecast_1)
is.na(forecast_1$rain_3h
is.na(forecast_1$rain_3h)
is.na(forecast_1$rain_3h)
ifelse('rain_3h' %in% colnames(forecast_1),
forecast_1$rain_3h[is.na(forecast_1$rain_3h)]<-0,forecast_1)
forecast_1$rain_3h[is.na(forecast_1$rain_3h)]
if(length(location) > 1){
forecast<-forecast
for (i in 2:length(location)) {
forecast_1<-owmr::get_forecast(location[i], units = "metric")%>%
owmr_as_tibble()%>%
tibble::add_column(sub_region_1= location[i], .after = "dt_txt")
forecast_1$rain_3h[is.na(forecast_1$rain_3h)]<-0
forecast<-merge(forecast,forecast_1, all=T)
}
}
forecast_1$rain_3h[is.na(forecast_1$rain_3h)]
forecast_1$rain_3h
#If the location vector is above 1, it adds the additional location data into the forecasting data frame.
if(length(location) > 1){
forecast<-forecast
for (i in 2:length(location)) {
forecast_1<-owmr::get_forecast(location[i], units = "metric")%>%
owmr_as_tibble()%>%
tibble::add_column(sub_region_1= location[i], .after = "dt_txt")
ifelse('rain_3h' %in% colnames(forecast_1),
forecast_1$rain_3h[is.na(forecast_1$rain_3h)]<-0,)
forecast<-merge(forecast,forecast_1, all=T)
}
}
forecast_1$rain_3h[is.na(forecast_1$rain_3h)]
colnames(forecast_1)
setwd("C:/Users/mlj206/Github/parksinthepandemic")
# Set the root directory as the working directory
# Libraries
# ---------
library(rgdal)
#library(leaflet)
library(data.table)
library(tidyverse)
library(shinydashboard)
library(shinythemes)
library(shiny)
#library(shinyWidgets)
#library(leaflet.extras)
library(dashboardthemes)
library(htmlwidgets)
library(htmltools)
library(remotes)
library(osfr)
library(here)
library(conflicted)
library(randomForest)
conflict_prefer("box", "shinydashboard")
# Import functions from repo
# --------------------------
source("code/plot.googlemobilitydistricts.R")
source("code/read.googlemobility.R")
source("code/getRnumbers.R")
source("code/plot.parkvisits.R")
# Read data
# ----------
googleandmetoffice_england<-read.csv('data/temporal/googleandmetoffice_england.csv')
RF_model<-readRDS('data/model/RF_model.RDS')
forecast_england<-read.csv('data/model/forecasts_england.csv')
# make the map
#bedford <- readOGR(dsn="data/spatial", layer="TL_GreenspaceSite")
#shapeData <- spTransform(bedford, CRS("+init=epsg:4326"))
#UK_latlon <- readRDS("data/UK_dat_ggplot.RDS")
#UK_Mobility <- readRDS("data/UK_Mobility.RDS")
shapeData <- readOGR(dsn="data/spatial", layer="googleboundaries_WGS84")
#shapeData$NAME <- gsub( " *\\(.*?\\) *", "", shapeData$NAME)
#shapeData$NAME <- gsub( "City of ", "", shapeData$NAME)
#shapeData$NAME <- gsub( "The Brighton and Hove", "Brighton and Hove", shapeData$NAME)
Rnumbers <- getRnumbers()
# Widgets
# -------
# this layout is very tidy
text.box <- box(title = "How busy are my local parks likely to be?", footer = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In suscipit malesuada dolor, ac auctor mi venenatis nec. Suspendisse ipsum augue, luctus venenatis sagittis sit amet, posuere non velit. Praesent aliquam finibus consectetur. Phasellus laoreet nisi tincidunt lorem fringilla tincidunt. Fusce quis pulvinar ipsum, a blandit nisl. Sed quis est rhoncus, porta nunc a, cursus tellus. Morbi euismod erat felis. Sed varius quam vel eros congue, vel convallis justo ullamcorper. Maecenas posuere, justo sed dapibus posuere, leo ex dapibus est, id viverra neque odio in lorem. Proin lobortis ante est, sed aliquet orci varius sed.", width = 12)
date.box <- dateRangeInput("daterange1", "Date range:", start="2020-01-01", end="2021-01-01")
day.box<-selectInput("dayOfTheWeek", "Choose a day of the week", choices=c("Sunday"='1',"Monday"='2',"Tuesday"='3', "Wednesday"='4', "Thursday"='5', "Friday"='6', "Saturday"='7'),
selected=data.table::wday(as.Date(Sys.Date())), multiple = FALSE, selectize = TRUE, width=NULL, size=NULL)
place.box<-selectInput("place", "Choose a region", choices=unique(shapeData$Mblty_n)
, selected = "Bedford", multiple = FALSE, selectize = TRUE, width = NULL, size = NULL)
baseline.check<-selectInput("custom_base", "Do you want a custom baseline?", choices=c("No", "Yes"), selected = "No")
#baseline.box<-dateInput("basedate", "Date:", value = "2020-02-29")
graph <- plotOutput("plot1")
graph2<-plotOutput("plot2")
map <- plotOutput("map1", height=700*1.5, width=400*1.5)
info.box <-infoBox("R value", value = paste0("England's R number is ", Rnumbers$Rnumbers.R_med[1]), subtitle = NULL,
icon = shiny::icon("bar-chart"), color = "aqua", width = 4,
href = TRUE, fill = FALSE)
#map <- leafletOutput("map1", height = 600)
# UI.R code
# ---------
header <- dashboardHeader(title="Parks in the Pandemic", titleWidth = 250)
sidebar <- dashboardSidebar(date.box,place.box, day.box,baseline.check,
conditionalPanel("input.custom_base=='Yes'",
dateInput("basedate", "Date:", value = "2020-02-29")),
width = 250)
body <- dashboardBody(
tags$head(tags$style(HTML('
/* main sidebar */
.skin-blue .main-sidebar {
background-color: #308759;
}
/* logo */
.skin-blue .main-header .logo {
background-color: #308759;
}
/* navbar (rest of the header) */
.skin-blue .main-header .navbar {
background-color: #308759;
}
/* body */
.content-wrapper, .right-side {
background-color:	#FFFFFF ;
}
'))),
fluidRow(
column(
6,
text.box,
box(width=6, graph),
box(width=6, graph2),
info.box
),
column(6, box(width=12, map))
)
)
# server
# ------
server <- function(input, output) {
google_react<-reactive({
googleandmetoffice_england %>%
dplyr::filter(dplyr::between(as.Date(date),min(as.Date(input$daterange1)), max(as.Date(input$daterange1)))) %>%
select(sub_region_1, date, parks_percent_change_from_baseline) %>%
group_by(sub_region_1) %>%
dplyr::summarise(mn=mean(parks_percent_change_from_baseline, na.rm=TRUE)) %>%
dplyr::mutate(Mblty_n=sub_region_1)
})
google_react2<-reactive({
googleandmetoffice_england %>%
dplyr::filter(dplyr::between(as.Date(date),min(as.Date(input$daterange1)), max(as.Date(input$daterange1)))) %>%
dplyr::filter(sub_region_1==input$place)
})
shapeData2<-reactive({
shapeData2 <- shapeData[shapeData$Mblty_n==input$place,]
})
google_shp_merge<-reactive({merge(shapeData, google_react())})
#map
output$map1<-renderPlot({
par(mar=c(3, 0, 3, 0))
myscale<-grDevices::colorRampPalette(colors = c("darkgrey", "white", "darkgreen"))
mycolours<-myscale(8)
mybreaks <- c(-60,-40,-20,0, 20,40,60)
#cut(google_shp_merge$mn, mybreaks)
mycolourscheme <- mycolours[findInterval(google_shp_merge()$mn, vec = mybreaks)]
plot(google_shp_merge(), xlim=c(-5.5,1.5), ylim=c(50,54.1), col = mycolourscheme)
plot(shapeData2(), xlim=c(-5.5,1.5), ylim=c(50,54.1), add=TRUE, lwd=2, border='purple')
})
#plot
output$plot1<-renderPlot({
print(plot.googlemobilitydistricts(google_react2(), "parks", print(input$place)))})
output$plot2<-renderPlot({
print(plot.parkvisits(googleandmetoffice = googleandmetoffice_england, model=RF_model, forecast = forecast_england,
district = input$place, dayofweek =as.numeric(input$dayOfTheWeek)))
#plot.parkvisits(googleandmetoffice = googleandmetoffice, model=model, forecast = forecast, district = district,dayofweek = dayofweek)
})
#example plot function - MATT YOU WILL NEED TO EDIT IT TO MAKE IT REACTIVE TO DISTRICTS AND WEEKDAY
#plot.parkvisits(googleandmetoffice = googleandmetoffice_england,
#model = RF_model,
#forecast = forecast_england)
}
# Run
# ---
shinyApp(
ui = dashboardPage(header, sidebar, body),
server = server
)
